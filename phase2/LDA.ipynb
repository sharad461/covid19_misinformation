{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlqp_9XgzDkn"
   },
   "outputs": [],
   "source": [
    "# One time run\n",
    "# gensim stopped Mallet LDA wrapper support past 3.8.3\n",
    "\n",
    "! pip install gensim==3.8.3 spacy pyLDAvis nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One time run\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, pickle\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The idea is to load in all dataset files and extract samplepct (declared below) percent of tweets and\n",
    "# store the rest in another directory for unseen data evaluation later on\n",
    "\n",
    "sentiments_dir = \"sentiments\" # Input files directory, all sentiment files\n",
    "eval_files_dir = \"output\" # Test files (the other 80 pc) directory\n",
    "samplepct = 0.04 # 4 percent\n",
    "\n",
    "source_hashtags = [\"coronavirus\", \"covid19\", \"coronaoutbreak\"]\n",
    "# Lowercasing #Coronavirus, #COVID19, #CoronaOutbreak because we lowercase all tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(lst, n):\n",
    "    '''\n",
    "    Chunks a python list object into smaller lists with n elements each\n",
    "    '''\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "        \n",
    "def lemmatize(data):\n",
    "    '''\n",
    "    Lemmatizes a chunk of tweets\n",
    "    Processes 10000 tweets at a time with batches of size 5000 (takes around 27s / 10000 tweets on my device)\n",
    "    '''\n",
    "    lemmatized = []\n",
    "    chunksize = 10000\n",
    "    \n",
    "    for chunk in partition(data, chunksize):\n",
    "        processed = list(nlp.pipe(chunk, batch_size=5000)) # running the Spacy pipeline\n",
    "\n",
    "        for doc in processed:\n",
    "            final = \" \".join([token.lemma_ for token in doc]) # token.lemma_\n",
    "            lemmatized.append(final)\n",
    "        del processed\n",
    "    \n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = {k:\"\" for k in stopwords.words(\"english\")} # Making a dictionary of stopwords for O(1) lookup\n",
    "\n",
    "rhashtag = re.compile(r\"(^|[^\\#\\w])\\#\\s?(\\w+)\\b\") # Compiling regex expressions to match/find later\n",
    "rurl = re.compile(r\"http\\S+\")\n",
    "\n",
    "def clean_tweet(x):\n",
    "    '''\n",
    "    Removing URLs, tweet handles and English stopwords from the text\n",
    "    '''\n",
    "    x = re.sub(rurl, '', x)\n",
    "    x = re.sub(rhashtag, '', x)\n",
    "    x = ' '.join([word for word in x.strip().split() if word not in stop_words])\n",
    "    return x\n",
    "\n",
    "def profile_tweet(x):\n",
    "    '''\n",
    "    Profiling tweets in a dictionary based on hashtags\n",
    "    \n",
    "    Input: list of tweets\n",
    "    Output: dictionary with hashtags as keys and pseudodocs as values\n",
    "    '''\n",
    "    hashtags = list(set(re.findall(rhashtag, x)))\n",
    "    x = clean_tweet(x)\n",
    "    for b, hashtag in hashtags:\n",
    "        if hashtag not in source_hashtags:\n",
    "            if hashtag in tweets:\n",
    "                tweets[hashtag].append(x)\n",
    "            else:\n",
    "                tweets[hashtag] = [x]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Spacy model disabling parser, named entity recognizer and text categorizer\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"textcat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file sentiments_001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sharad.duwal\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3169: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE ORIGINAL TWEET: #StayAtHomeSaveLives Don't get paidðŸ¤¬ Bills piling up e.g house rent etc. Then the next BIG thing is STARVATION. UK GOVERNMENT help us too...We can't go out or even shop for food online. All delivery DATES ARE GONE ðŸš¨ We are HUNGRYðŸ˜­ #UKlockdown #Coronavirus #UKGovtHelpUsToo\n",
      "SMALL CASED AND WITHOUT STOPWORDS: n't get pay ðŸ¤¬ bill pile e.g house rent etc . next big thing starvation . uk government help ... ca n't go even shop food online . delivery date go ðŸš¨ hungry ðŸ˜­\n",
      "number of pseudodocs (total): 12650\n",
      "==============================================================================\n",
      "processing file sentiments_002.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SHARAD~1.DUW\\AppData\\Local\\Temp/ipykernel_8244/3234218283.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"processing file {file}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentiments_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# reading a sentiments file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mslice_at\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamplepct\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sharad.duwal\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sharad.duwal\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sharad.duwal\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1055\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1057\u001b[1;33m         \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1058\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sharad.duwal\\miniconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2059\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2060\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2061\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2062\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2063\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sharad.duwal\\miniconda3\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m     \"\"\"\n\u001b[0;32m    539\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tweets = {} # Dictionary which will have \"{hashtag}\": \"{all tweets as a single text for the hashtag}\", relevant function: profile_tweet()\n",
    "\n",
    "# Loading in the dataset files, extract samplepct percent of tweets,\n",
    "# lemmatize them, small case them, profile them into pseudodocs\n",
    "# and remove hashtags and URLs from the tweets, then finally\n",
    "# save (100 - samplepct) percent file into eval_files_dir directory\n",
    "\n",
    "for file in os.listdir(sentiments_dir):\n",
    "    print(f\"processing file {file}\")\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(sentiments_dir, file))[[\"id\", \"text\"]] # reading a sentiments file\n",
    "    slice_at = int(samplepct * df.shape[0])\n",
    "    \n",
    "    train = df.iloc[:slice_at].text\n",
    "    test = df.iloc[slice_at:]\n",
    "    \n",
    "    print(f\"EXAMPLE ORIGINAL TWEET: {train[14]}\")\n",
    "    \n",
    "    texts = train.apply(lambda x: x.lower()) # running lowercase\n",
    "    texts = pd.DataFrame(lemmatize(texts), columns=[\"text\"]).text.apply(profile_tweet) # preprocess and profile all tweets\n",
    "    \n",
    "    print(f\"SMALL CASED AND WITHOUT STOPWORDS: {texts[14]}\")\n",
    "    \n",
    "    test.to_csv(os.path.join(eval_files_dir, file), index=False)\n",
    "    \n",
    "    print(f\"number of pseudodocs (total): {len(tweets)}\")\n",
    "    print(\"==============================================================================\")\n",
    "    \n",
    "    assert len(train) + len(test) == df.shape[0], \"Something is wrong because this should be equal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pseudodocs dictionary so that we don't have to partition train/test each time\n",
    "pickle.dump(tweets, open(\"pseudo.docs\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load the pseudodocs (on a new/fresh run)\n",
    "tweets = pickle.load(open(\"0.04/pseudo.docs\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "iGf0B-tvzDkx"
   },
   "outputs": [],
   "source": [
    "# Joining the profiled tweet lists into a single string, pseudodoc\n",
    "for hashtag in tweets:\n",
    "    tweets[hashtag] = ' '.join(tweets[hashtag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2152379, 'covid_19')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max((len(v), k) for k, v in tweets.items()) # The longest pseudodoc and its size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetslist, hashtags = [], []\n",
    "wordlimit = 200000 # Token limit, the pseudodocs will be capped at 200000 tokens\n",
    "\n",
    "for hashtag, text in tweets.items():\n",
    "    hashtags.append(hashtag)\n",
    "    tweetslist.append(text[:wordlimit])\n",
    "\n",
    "# Quick check\n",
    "assert len(tweetslist) == len(hashtags), \"Number of pseudodocs and hashtags unequal, look into it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319658"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercambio\n",
      "ðŸ“£ new podcast ! \" crisis schooling begin \" @spreaker ' I think hoax ' : patient 30 die attend ' covid party ' hold valid permit , approve study permit , restriction take effect march 18 , 2020 travel u offer emergency grant student affect by-19 | : ireland put lockdown varadkar ask nation ' forego freedom '\n"
     ]
    }
   ],
   "source": [
    "# Display a sample hashtag and its pseudodoc\n",
    "ix = 1001 # pseudodoc number\n",
    "\n",
    "print(hashtags[ix])\n",
    "if len(tweetslist[ix]) < 10000:\n",
    "    print(tweetslist[ix])\n",
    "else:\n",
    "    print(\"selected pseudodoc too long to print on screen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "59fs3GBLzDkz"
   },
   "outputs": [],
   "source": [
    "data = tweetslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12650"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) # No of pseudodocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FYD5XBhRzDk1",
    "outputId": "9aa76083-66dc-4d70-b599-e503b2def3b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['university', 'queensland', 'highly', 'anticipate', 'covid', 'vaccine', 'pass', 'another', 'crucial', 'milestone', 'week', 'bring', 'one', 'step', 'close', 'towards', 'become', 'reality', 'screen', 'announce', 'support', 'virtual', 'live', 'screen', 'culture', 'event']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize words after simple preprocessing\n",
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return (final)\n",
    "\n",
    "data_words = gen_words(data)\n",
    "\n",
    "print (data_words[700][0:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2pSJxf6OzDk3",
    "outputId": "2fe07d40-f23a-42d9-ea24-6834b61133b9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(13, 1), (56, 1), (128, 1), (129, 1), (174, 1), (205, 1), (273, 1), (323, 1), (452, 1), (730, 1), (853, 1), (879, 1), (1181, 1), (1307, 1), (1319, 1), (1353, 1), (1619, 1), (1991, 1), (2012, 1), (2074, 2), (2767, 1), (2950, 1), (3006, 1), (3161, 1), (3205, 1)]\n",
      "heure\n"
     ]
    }
   ],
   "source": [
    "# Word <> Id mapping\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "id2word.filter_extremes(no_below=5, no_above=0.5) # filter words that appear in less than 5 docs and more than 0.5 (half) of the docs\n",
    "\n",
    "corpus = []\n",
    "for text in data_words:\n",
    "    new = id2word.doc2bow(text)\n",
    "    corpus.append(new)\n",
    "\n",
    "print(corpus[700][0:40])\n",
    "print(id2word[2]) #(2, 3) below means id2word[2] appears 3 times in the doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following are statements to save the python objects created so far\n",
    "# (to load later without having to repeat the processes)\n",
    "\n",
    "pickle.dump(data_words, open(\"data_words\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(corpus, open(\"corpus\", \"wb\")) # Save\n",
    "\n",
    "# corpus = pickle.load(open(\"corpus\", 'rb')) # Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(id2word, open(\"id2word\", \"wb\"))\n",
    "\n",
    "# id2word = pickle.load(open(\"id2word\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the lemmatized pseudodocs python object (to use later) and the LDA model, if necessary\n",
    "\n",
    "pickle.dump(data, open(\"lemmatized_texts.p\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOlmVvZwzDk4"
   },
   "outputs": [],
   "source": [
    "# lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "#                                            id2word=id2word,\n",
    "#                                            num_topics=10,\n",
    "#                                            random_state=100,\n",
    "#                                            update_every=1,\n",
    "#                                            chunksize=1000,\n",
    "#                                            passes=10,\n",
    "#                                            alpha=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Mallet LDA model\n",
    "\n",
    "path = 'C:/mallet/bin/mallet'\n",
    "mallet_model = LdaMallet(path, corpus=corpus, num_topics=10, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "lLaY8y9MzDk7"
   },
   "outputs": [],
   "source": [
    "# Convert Mallet LDA model to gensim LDA model\n",
    "# gensim LDA model has more features\n",
    "# The function creates a gensim LDA model with the weights of the original model\n",
    "\n",
    "model = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(mallet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model for later use\n",
    "\n",
    "model.save(\"lda.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "model = gensim.models.ldamodel.LdaModel.load(\"lda.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "FKIBjZDYzDk7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.4165980211523473\n"
     ]
    }
   ],
   "source": [
    "# Rank 1 metric search about\n",
    "\n",
    "# This score generally takes much longer to compute (very expensive on models trained on large sets)\n",
    "coherence_model_lda = CoherenceModel(model=model, texts=data_words, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "5Rhi8RYmzDk7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  -2.2953723826050436\n"
     ]
    }
   ],
   "source": [
    "# Coherence score using U Mass\n",
    "coherence_model_lda = CoherenceModel(model=model, corpus=corpus, coherence=\"u_mass\")\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "OMeyC-cszDk7"
   },
   "outputs": [],
   "source": [
    "# Generate topics of the model\n",
    "\n",
    "ldatopics = model.print_topics(num_topics=-1, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the topics readable\n",
    "\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_punctuation, strip_numeric\n",
    "\n",
    "topics = []\n",
    "filters = [lambda x: x.lower(), strip_punctuation, strip_numeric]\n",
    "\n",
    "for topic in ldatopics:\n",
    "    topics.append(preprocess_string(topic[1], filters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['day',\n",
       "  'pandemic',\n",
       "  'start',\n",
       "  'time',\n",
       "  'due',\n",
       "  'state',\n",
       "  'government',\n",
       "  'month',\n",
       "  'pay',\n",
       "  'plan',\n",
       "  'jul',\n",
       "  'crisis',\n",
       "  'country',\n",
       "  'economy',\n",
       "  'issue',\n",
       "  'trend',\n",
       "  'national',\n",
       "  'open',\n",
       "  'hit',\n",
       "  'run'],\n",
       " ['good',\n",
       "  'today',\n",
       "  'link',\n",
       "  'back',\n",
       "  'free',\n",
       "  'youtube',\n",
       "  'life',\n",
       "  'school',\n",
       "  've',\n",
       "  'offer',\n",
       "  'click',\n",
       "  'feel',\n",
       "  'ca',\n",
       "  'play',\n",
       "  'book',\n",
       "  'hope',\n",
       "  'make',\n",
       "  'daily',\n",
       "  'guy',\n",
       "  'amazing'],\n",
       " ['support',\n",
       "  'check',\n",
       "  'sign',\n",
       "  'gt',\n",
       "  'll',\n",
       "  'deliver',\n",
       "  'design',\n",
       "  'project',\n",
       "  'contact',\n",
       "  'market',\n",
       "  'base',\n",
       "  'official',\n",
       "  'event',\n",
       "  'man',\n",
       "  'high',\n",
       "  'track',\n",
       "  'group',\n",
       "  'listen',\n",
       "  'night',\n",
       "  'real'],\n",
       " ['work',\n",
       "  'home',\n",
       "  'stay',\n",
       "  'quarantine',\n",
       "  'safe',\n",
       "  'top',\n",
       "  'covid',\n",
       "  'social',\n",
       "  'video',\n",
       "  'lockdown',\n",
       "  'post',\n",
       "  'watch',\n",
       "  'read',\n",
       "  'people',\n",
       "  'distancing',\n",
       "  'today',\n",
       "  'amid',\n",
       "  'thing',\n",
       "  'time',\n",
       "  'life'],\n",
       " ['coronavirus',\n",
       "  'covid',\n",
       "  'news',\n",
       "  'virus',\n",
       "  'live',\n",
       "  'world',\n",
       "  'read',\n",
       "  'follow',\n",
       "  'outbreak',\n",
       "  'rt',\n",
       "  'late',\n",
       "  'pandemic',\n",
       "  'uk',\n",
       "  'vaccine',\n",
       "  'break',\n",
       "  'click',\n",
       "  'amp',\n",
       "  'government',\n",
       "  'country',\n",
       "  'spread'],\n",
       " ['amp',\n",
       "  'give',\n",
       "  'change',\n",
       "  'great',\n",
       "  'online',\n",
       "  'share',\n",
       "  'call',\n",
       "  'service',\n",
       "  'world',\n",
       "  'provide',\n",
       "  'report',\n",
       "  'join',\n",
       "  'experience',\n",
       "  'donate',\n",
       "  'full',\n",
       "  'leader',\n",
       "  'story',\n",
       "  'local',\n",
       "  'love',\n",
       "  'week'],\n",
       " ['case',\n",
       "  'death',\n",
       "  'covid',\n",
       "  'total',\n",
       "  'update',\n",
       "  'test',\n",
       "  'positive',\n",
       "  'recover',\n",
       "  'patient',\n",
       "  'report',\n",
       "  'number',\n",
       "  'corona',\n",
       "  'people',\n",
       "  'confirm',\n",
       "  'today',\n",
       "  'coronavirus',\n",
       "  'critical',\n",
       "  'india',\n",
       "  'die',\n",
       "  'day'],\n",
       " ['time',\n",
       "  'trump',\n",
       "  'family',\n",
       "  'realdonaldtrump',\n",
       "  'people',\n",
       "  'lose',\n",
       "  'president',\n",
       "  'china',\n",
       "  'year',\n",
       "  'friend',\n",
       "  'bad',\n",
       "  'test',\n",
       "  'house',\n",
       "  'share',\n",
       "  'make',\n",
       "  'happen',\n",
       "  'response',\n",
       "  'control',\n",
       "  'stand',\n",
       "  'thing'],\n",
       " ['mask',\n",
       "  'make',\n",
       "  'health',\n",
       "  'face',\n",
       "  'care',\n",
       "  'fight',\n",
       "  'people',\n",
       "  'protect',\n",
       "  'worker',\n",
       "  'spread',\n",
       "  'medical',\n",
       "  'wear',\n",
       "  'public',\n",
       "  'doctor',\n",
       "  'community',\n",
       "  'child',\n",
       "  'find',\n",
       "  'buy',\n",
       "  'staff',\n",
       "  'cover'],\n",
       " ['business',\n",
       "  'impact',\n",
       "  'covid',\n",
       "  'crisis',\n",
       "  'late',\n",
       "  'learn',\n",
       "  'company',\n",
       "  'join',\n",
       "  'service',\n",
       "  'continue',\n",
       "  'pandemic',\n",
       "  'discuss',\n",
       "  'response',\n",
       "  'register',\n",
       "  'challenge',\n",
       "  'webinar',\n",
       "  'job',\n",
       "  'article',\n",
       "  'virtual',\n",
       "  'pm']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the topics into topics.txt\n",
    "\n",
    "with open(\"topics.txt\", \"w\", encoding=\"utf8\") as f:\n",
    "    for topic in topics:\n",
    "        f.write(\" \".join(topic) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model (very expensive on models trained on large sets)\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(model, corpus, id2word, mds=\"mmds\", R=20) # 20 top relevant tokens\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RP-2Zpc6zDk8"
   },
   "outputs": [],
   "source": [
    "# Generate topics for unseen data\n",
    "# Will start working with the other 80 pc of data once we have a verified up and running LDA model\n",
    "# This is sample result\n",
    "\n",
    "unseen = [\"what are you doing?\", \"What's up bro?\"]\n",
    "\n",
    "\n",
    "for doc in gen_words(unseen):\n",
    "    vec = id2word.doc2bow(doc)\n",
    "    topics = model[vec]\n",
    "    for topic in topics:\n",
    "        print(model.show_topic(topic[0]), topic[1])\n",
    "    print(\"=================================================================================\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LDA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
